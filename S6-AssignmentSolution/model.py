# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s3D2efl2_hYQTICQvranvL_xijJ_rEq_
"""

import torch
import torch.nn as nn # for network
import torch.nn.functional as F # for forward method

drop_out_value = 0.1

class Network(nn.Module):
  def __init__(self, mode = None):
    super(Network,self).__init__() # extending super class method
    self.mode = mode
    print ("mode :", mode)

    # Input Block
    self.convblock1 = nn.Sequential(
        nn.Conv2d(1,12,3), # In- 1x28x28, Out- 12x26x26, RF- 3x3, Jump_in -1, Jump_out -1
        nn.ReLU()
        # ,
        # nn.BatchNorm2d(12),
        # nn.Dropout(drop_out_value)
    ) 

    if mode =="BN":
      self.norm1 = nn.BatchNorm2d(12) # BatchNorm
    if mode == "LN":
      self.norm1 = nn.LayerNorm([12,26,26]) # Layer Norm
    if mode == "GN":
      self.norm1 = nn.GroupNorm(2,12) # Group Norm

    self.dropout1 = nn.Dropout(drop_out_value) # drop out
    ###---------------------------------------------------

    # Conv Block 2
    self.convblock2 = nn.Sequential(
        nn.Conv2d(12,12,3), # In- 12x26x26, Out- 12x24x24, RF- 5x5, Jump_in -1, Jump_out -1
        nn.ReLU()
        # ,
        # nn.BatchNorm2d(12),
        # nn.Dropout(drop_out_value)
    ) 

    if mode =="BN":
      self.norm2 = nn.BatchNorm2d(12) # BatchNorm
    if mode == "LN":
      self.norm2 = nn.LayerNorm([12,24,24]) # Layer Norm
    if mode == "GN":
      self.norm2 = nn.GroupNorm(2,12) # Group Norm

    self.dropout2 = nn.Dropout(drop_out_value) # drop out
    ###---------------------------------------------------


    # Conv Block 3
    self.convblock3 = nn.Sequential(
        nn.Conv2d(12,12,3), # In- 12x24x24, Out- 12x22x22, RF- 7x7, Jump_in -1, Jump_out -1
        nn.ReLU()
        # ,
        # nn.BatchNorm2d(12),
        # nn.Dropout(drop_out_value)
    ) 

    if mode =="BN":
      self.norm3 = nn.BatchNorm2d(12) # BatchNorm
    if mode == "LN":
      self.norm3 = nn.LayerNorm([12,22,22]) # Layer Norm
    if mode == "GN":
      self.norm3 = nn.GroupNorm(2,12) # Group Norm

    self.dropout3 = nn.Dropout(drop_out_value) # drop out
    ###---------------------------------------------------

    self.convblock3_1 = nn.Sequential(
        nn.Conv2d(12,12,3), # In- 12x22x22, Out- 12x20x20, RF- 7x7, Jump_in -1, Jump_out -1
        nn.ReLU()
        # ,
        # nn.BatchNorm2d(12),
        # nn.Dropout(drop_out_value)
    ) 

    if mode =="BN":
      self.norm3_1 = nn.BatchNorm2d(12) # BatchNorm
    if mode == "LN":
      self.norm3_1 = nn.LayerNorm([12,20,20]) # Layer Norm
    if mode == "GN":
      self.norm3_1 = nn.GroupNorm(2,12) # Group Norm

    self.dropout3_1 = nn.Dropout(drop_out_value) # drop out
    ###---------------------------------------------------

    self.convblock3_2 = nn.Sequential(
        nn.Conv2d(12,12,3), # In- 12x20x20, Out- 12x18x18, RF- 7x7, Jump_in -1, Jump_out -1
        nn.ReLU()
        # ,
        # nn.BatchNorm2d(12),
        # nn.Dropout(drop_out_value)
    )

    if mode =="BN":
      self.norm3_2 = nn.BatchNorm2d(12) # BatchNorm
    if mode == "LN":
      self.norm3_2 = nn.LayerNorm([12,18,18]) # Layer Norm
    if mode == "GN":
      self.norm3_2 = nn.GroupNorm(2,12) # Group Norm

    self.dropout3_2 = nn.Dropout(drop_out_value) # drop out
    ###---------------------------------------------------
    

    #### Transition Block 1 
    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # In- 12x18x18 Out- 12x9x9 RF- 8x8, Jump_in -1, Jump_out -2


    # Conv Block 5
    self.convblock5 = nn.Sequential(
        nn.Conv2d(12,16,3), # In- 12x9x9 Out- 16x7x7 RF- 12x12, Jump_in -2, Jump_out -2
        nn.ReLU()
        # ,
        # nn.BatchNorm2d(16),
        # nn.Dropout(drop_out_value)
    ) 

    if mode =="BN":
      self.norm5 = nn.BatchNorm2d(16) # BatchNorm
    if mode == "LN":
      self.norm5 = nn.LayerNorm([16,7,7]) # Layer Norm
    if mode == "GN":
      self.norm5 = nn.GroupNorm(2,16) # Group Norm

    self.dropout5 = nn.Dropout(drop_out_value) # drop out
    ###---------------------------------------------------

    # Conv Block 6
    self.convblock6 = nn.Sequential(
        nn.Conv2d(16,16,3), # In- 16x7x7 Out- 16x5x5 RF- 16x16, Jump_in -2, Jump_out -2
        nn.ReLU()
        # ,
        # nn.BatchNorm2d(16),
        # nn.Dropout(drop_out_value)
    ) 

    if mode =="BN":
      self.norm6 = nn.BatchNorm2d(16) # BatchNorm
    if mode == "LN":
      self.norm6 = nn.LayerNorm([16,5,5]) # Layer Norm
    if mode == "GN":
      self.norm6 = nn.GroupNorm(2,16) # Group Norm

    self.dropout6 = nn.Dropout(drop_out_value) # drop out
    ###---------------------------------------------------

    # Output Block
    self.convblock7 = nn.Sequential(
        nn.Conv2d(16,10,1), # In- 16x5x5 Out- 10x5x5, RF- 16x16, Jump_in -2, Jump_out -2
        nn.ReLU()
        # ,
        # nn.BatchNorm2d(10),
        # nn.Dropout(drop_out_value)
    ) 

    if mode =="BN":
      self.norm7 = nn.BatchNorm2d(10) # BatchNorm
    if mode == "LN":
      self.norm7 = nn.LayerNorm([10,5,5]) # Layer Norm
    if mode == "GN":
      self.norm7 = nn.GroupNorm(2,10) # Group Norm

    self.dropout7 = nn.Dropout(drop_out_value) # drop out
    ###---------------------------------------------------

    self.gap = nn.AvgPool2d(5) # In- 10x5x5, Out- 10x1x1, RF- 16x16, Jump_in -2, Jump_out -2

  
  def forward(self,x):
  
    x = self.convblock1(x)
    x = self.norm1(x)
    x = self.dropout1(x)


    x = self.convblock2(x)
    # if mode: 
    x = self.norm2(x)
    x = self.dropout2(x)
  

    x = self.convblock3(x)
    # if mode: 
    x = self.norm3(x)
    x = self.dropout3(x)

    x = self.convblock3_1(x)
    # if mode: 
    x = self.norm3_1(x)
    x = self.dropout3_1(x)

    x = self.convblock3_2(x)
    # if mode: 
    x = self.norm3_2(x)
    x = self.dropout3_2(x)

    x = self.pool1(x)

    x = self.convblock5(x)
    # if mode: 
    x = self.norm5(x)
    x = self.dropout5(x)


    x = self.convblock6(x)
    # if mode: 
    x = self.norm6(x)
    x = self.dropout6(x)

    x = self.convblock7(x)
    # if mode: 
    x = self.norm7(x)
    x = self.dropout7(x)


    x = self.gap(x)

    # Flattening
    x = x.view(-1,10)
    return F.log_softmax(x,dim=-1)

# model = Network(mode="GN")
# print(model)